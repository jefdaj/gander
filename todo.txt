features:
  precompiled binary release for mac + linux
  efficiency:
    strict strings/bytestrings for hashes?
    better map in dupemap is probably important on big datasets
      data.map.strict or .lazy don't seem to help on small examples
        according to https://wiki.haskell.org/GHC/Memory_Footprint,
          there should be a savings of ~4.5 vs 6
      judy arrays? require Word keys, so no
      bytestring-trie?
    would doing strict tree operations speed it up?
      no! prevents streaming hashes
    what about strict tree constructors?
      small benefit? actually decreases performance a bit so far
    use filterMap instead of any lists when finding lost files
    make maps immediately rather than using fromListWith in DupeMap?
    does reading hashes take too much memory? if so try https://hackage.haskell.org/package/streaming-utils

timing data:
  time gander hash ~/nixpkgs/ +RTS -p > test-hashes.txt
    strictmap hashes 17m48s (final hash 59fb4ec51)
    bytestrings hashes 7m6s (final hash 59fb4ec51) ... wait that was without -p :(
    bytestrings dupes 4m53s (no -p)
    bytestrings dupes with string tree fields 4m48s (no -p)
    bytestrings dupes with string tree fields 7m39s (with -p)
    bytestrings dupes with strict constructors + stict map 10m17s (with -p)
    bytestrings dupes with strict constructors 9m36s (with -p), so remove them!

bugs:
  wrong counts for empty dirs:
    # 81 dirs (0 files each, 11 total) have hash e3b0c442
  crashes on hashing a symlink... but is that what we want anyway?
  edited 'folder1/folder2/file2.txt/file2.txt' (a86b253f)
  messed up root path to the annex (includes /.git at the end)
  when `annex hash`ing, need to remove the annex dir itself from path before building tree
    tree is already written that way
    ok after first init step
    but broken after annex add backup
    gander <annex> hash can fix it by updating hashes properly

proper testing:
  start with an actual dir of user-provided files we're allowed to mess up
  read the tree
  generate random deltas, simulate their results, and check that running them matches


x hashes.txt should not be included in itself
shouldn't use the top level dir name, only the forest of thngs hashed?
annexing new files always fails?
how to handle adding other annexes? copy links, ignore the hidden stuff

better hashing:
  should be fast to verify dir hashes when reading the tree right? might prevent file editing mistakes
  should there be a third L type for symlinks?

bugs to fix:
  dir hashes need to be recalculated after changing contents (including rm)
  have to handle "permission denied" errors? maybe just abort and make user have sudo

simplify the interface:
  gander hash  <target> [-e <exclude>]
  gander dupes <target> [-e <exclude>]
  gander dedup <target> [-e <exclude>] [-fc]
  gander diff  <oldtarget> <newtarget> [-e <exclude>]
  gander test  <testdir>
  gander <annex> init
  gander <annex> hash  [<path>] [-f]
  gander <annex> dupes [<path>]
  gander <annex> dedup [<path>] [-fc]
  gander <annex> add    <path>  [-s <srcpath>] [-fc]
  gander <annex> rm     <path>  [-fc]

minor features to add if time:
  make sure dir listings are alphabetical (probably are already)
  warning when you use potentially destructive standalone commands in an annex
  warning if it looks like you're trying to delete the root dir!
