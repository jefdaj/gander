decorated trees:
  x 1. add a decoration type to File constructor, but specialize everything using it to ()
  x 2. swap out () for any type a, but keep it in production functions
  3. add test functions that replace () with B8.ByteString

should the top level dir include its absolute path?
  if so, have to allow slashes

forest test properties:
  reading multiple trees into a forest should be the same as reading them separately and making a list
  should the empty forest have the same hash as the empty tree?
  you should be able to concat serialized trees and read them back as a forest
  if you make a forest with a tree duplicated N times, the top dupe should be that whole tree N times
    should it also be the only dupe, since everything else is redundant?
    it should work even if you rename the top dir

tree test properties:
  tree should maintain the same hashes if you randomly rename some things, right?
  diff of the same tree twice should be empty
  if you write the read/show instances right, the printed description of a diff should equal the list of deltas that generated it?

feature:
  dupes should have arguments for "min dedup n files to report" and "max number of dedups to report"
  dupes across multiple hashes files would make dealing with lots of drives much easier
  annex mode would be less confusing if it required an explicit --annex flag
  add .git/* to the default ignore list?
  when using max depth, dupes should still properly report the total number of files
    while you're at it, dirs should probably also count as files because they take up inodes and its more consistent
    add a Skip Int data type while parsing somewhere, and use it to add to the dir counts?
    test that total nFiles is the same no matter what max depth you parse with
    how should you handle serializing a tree with skips in it? test that it also round-trips successfully

convert to ST:
  x pathsByHash
  dupesByNFiles
  simplifyDupes
  sortDupePaths

memory optimization:
  0. start from real hash data
  1. profile to figure out which step is consuming the most
       surprisingly, a loooot of (:), probably inside DupeMap somewhere?
  2a. if HashTree, think about removing it and going straight from hashes -> dupemap
  2b. if DupeMap, maybe optimize with bytestring-trie or judy array

features:
  sort dupe sets before printing! can't compare/diff otherwise
  precompiled binary release for mac + linux
  hash efficiency:
    strict strings/bytestrings for hashes?
    would doing strict tree operations speed it up?
      no! prevents streaming hashes
    what about strict tree constructors?
      small benefit? actually decreases performance a bit so far
    parallel strategies instead of monad-parallel? want only as many threads as cores
    convert directory.tree to bytestrings too?
  dupes efficiency:
    better map in dupemap is probably important on big datasets
      data.map.strict or .lazy don't seem to help on small examples
        according to https://wiki.haskell.org/GHC/Memory_Footprint,
          there should be a savings of ~4.5 vs 6
      judy arrays? require Word keys, so no
      bytestring-trie?
    use filterMap instead of any lists when finding lost files
    make maps immediately rather than using fromListWith in DupeMap?
    does reading hashes take too much memory?
      if so try https://hackage.haskell.org/package/streaming-utils
    prune bytestrings as suggested
    x save hashes.bin too and read it instead when possible

timing data:
  time gander hash ~/nixpkgs/ +RTS -p > test-hashes.txt
    strictmap hashes 17m48s (final hash 59fb4ec51)
    bytestrings hashes 7m6s (final hash 59fb4ec51) ... wait that was without -p :(

    bytestrings dupes 4m53s (no -p)
    bytestrings dupes with string tree fields 4m48s (no -p)
    bytestrings dupes with string tree fields 7m39s (with -p)
    bytestrings dupes with strict constructors + stict map 10m17s (with -p)
    bytestrings dupes with strict constructors 9m36s (with -p), so remove them!

    feature-strictmap-bytestrings-dupesets hashes 16m13s with -p (final hash 59fb4ec51)
      14m10s without -p
    feature-strictmap-bytestrings-dupesets dupes 11m1s

bugs:
  reading a forest sometimes only gets the first tree?
  errors that cause crashes in real-life situations:
    encountering a file with "invalid encoding" (possibly a hard drive error?)
      nope, this is a filename issue! start a dir of difficult test cases to actually scan and fix
    encountering a (nonexistent?) socket file. would be nice to specifically alert the user about that, or skip it
  wrong counts for empty dirs:
    # 81 dirs (0 files each, 11 total) have hash e3b0c442
  crashes on hashing a symlink... but is that what we want anyway?
  edited 'folder1/folder2/file2.txt/file2.txt' (a86b253f)
  messed up root path to the annex (includes /.git at the end)
  when `annex hash`ing, need to remove the annex dir itself from path before building tree
    tree is already written that way
    ok after first init step
    but broken after annex add backup
    gander <annex> hash can fix it by updating hashes properly

proper testing:
  start with an actual dir of user-provided files we're allowed to mess up
  read the tree
  generate random deltas, simulate their results, and check that running them matches


x hashes.txt should not be included in itself
shouldn't use the top level dir name, only the forest of thngs hashed?
annexing new files always fails?
how to handle adding other annexes? copy links, ignore the hidden stuff

better hashing:
  should be fast to verify dir hashes when reading the tree right? might prevent file editing mistakes
  should there be a third L type for symlinks?

bugs to fix:
  dir hashes need to be recalculated after changing contents (including rm)
  have to handle "permission denied" errors? maybe just abort and make user have sudo

simplify the interface:
  gander hash  <target> [-e <exclude>]
  gander dupes <target> [-e <exclude>]
  gander dedup <target> [-e <exclude>] [-fc]
  gander diff  <oldtarget> <newtarget> [-e <exclude>]
  gander test  <testdir>
  gander <annex> init
  gander <annex> hash  [<path>] [-f]
  gander <annex> dupes [<path>]
  gander <annex> dedup [<path>] [-fc]
  gander <annex> add    <path>  [-s <srcpath>] [-fc]
  gander <annex> rm     <path>  [-fc]

minor features to add if time:
  make sure dir listings are alphabetical (probably are already)
  warning when you use potentially destructive standalone commands in an annex
  warning if it looks like you're trying to delete the root dir!
